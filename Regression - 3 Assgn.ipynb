{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb6f25ba-4a43-43e4-8dc6-0d0e02406fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1.\n",
    "\n",
    "# Ridge regression adds a regularization term (L2 penalty) to the cost function.\n",
    "# It's used to address multicollinearity and prevent overfitting.\n",
    "# It introduces a trade-off between bias and variance\n",
    "# Ridge shrinks regression coefficients, making them more stable.\n",
    "# It has a hyperparameter to control the strength of regularization.\n",
    "# Mathematically, it minimizes the sum of squared residuals plus lambda times the sum of squared coefficients.\n",
    "# It is suitable when multicollinearity is present and a balance between complexity and fit is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "994f64db-212e-45d3-848b-a2e3b671345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.\n",
    "\n",
    "# Ridge regression assumes linearity b/w predictors and the response variable.\n",
    "# It assumes that there is no perfect multicollinearity among predictors.\n",
    "# The errors should follow a normal distribution.\n",
    "# Homoscedasticity is assumed, meaning constant variance of errors.\n",
    "# Independence of observations is assumed (no autocorrelation).\n",
    "# Ridge does not require the assumption of strict exogeneity.\n",
    "# The assumptions of linearity and constant variance are less critical due to regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c97b1c20-b423-4b4d-a02b-fe4f3a6b1502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.\n",
    "\n",
    "# We can use cross-validation techniques like k-fold cross-validation to find the optimal lambda.\n",
    "# Calculate the mean squared error or another suitable metric for different lambda values.\n",
    "# Choose the lambda that minimizes the cross-validated error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10fc7589-81d0-424c-ba43-d779ecf5c091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4.\n",
    "\n",
    "# Ridge Regression is primarily used to prevent overfitting by adding a regularization term to the cost function. While it can encourage some feature coefficients to become exactly zero, this is a byproduct of the regularization, not its primary purpose. For feature selection, methods like Lasso Regression are more suitable, as they explicitly drive some coefficients to zero.\n",
    "# Nevertheless, Ridge Regression can be used for feature selection.\n",
    "# It includes all features but shrinks their coefficients towards zero.\n",
    "# The magnitude of the lambda determines the degree of shrinkage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0165977-ec85-41b0-b2c5-9003e31d3c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5.\n",
    "\n",
    "# Ridge regression can handle multicollinearity effectively:\n",
    "\n",
    "# Multicollinearity occurs when predictor variables are highly correlated.\n",
    "# Ridge regression adds a penalty term that shrinks the coeffcients, reducing their sensitivity to multicollinearity.\n",
    "# It is effective in stabilizing coefficients, but it will not help identify which predictor variables are most important.\n",
    "# Ridge regression is better suited for improving model generalization in the presence of multicollinearity rather than feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bc32e13-cb5a-4a64-a9c0-6cd3afc7f295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6.\n",
    "\n",
    "# Ridge regression can handle both categorical and continuous independent variables:\n",
    "\n",
    "# Categorical variables should be one-hot encoded or converted to numerical values.\n",
    "# Ridge regression does not distinguish b/w the two types and can include them in the model.\n",
    "# Proper encoding is crucial to ensure meaningful results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49560add-48f1-4ba3-bf3e-9bb77e6f8896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7.\n",
    "\n",
    "# Interpreting coefficients in Ridge Regression:\n",
    "\n",
    "# Coefficients represent the relationship between independent and dependent variables.\n",
    "# In Ridge, coefficients are shrunk towards zero to reduce overfitting.\n",
    "# A positive coefficient means an increase in the predictor leads to an increase in the response.\n",
    "# A negative coefficient implies a decrease in the predictor results in a decrease in the response.\n",
    "# The magnitude of the coefficient shows its impact on the response.\n",
    "# Coefficients' values are adjusted by the regularization parameter (lambda).\n",
    "# Smaller lambda leads to coefficients closer to those in linear regression.\n",
    "# Ridge coefficients don't become exactly zero, unlike Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7efcd1a0-4584-4ca7-a867-83d286b9ae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8.\n",
    "\n",
    "# Yes, Ridge Regression can be used for time-series data.\n",
    "# In time-series analysis, predictors may include lagged variables, seasonal factors, etc.\n",
    "# Ridge can help reduce overfitting by shrinking coefficients, suitable for noisy time series.\n",
    "# Choosing the right predictors and tuning lambda are key for effective use in time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d20ddc7-5c40-4883-816e-bc24836f7de6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
